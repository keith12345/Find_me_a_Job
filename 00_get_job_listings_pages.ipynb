{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    driver = webdriver.Chrome(executable_path='/anaconda3/bin/chromedriver')\n",
    "    driver.get(url)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    driver.close()\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = get_soup('https://www.indeed.com/jobs?q=data+scientist&l=New+York')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(range(0,1000,10)):\n",
    "    base_url = 'https://ca.indeed.com/jobs?q={}&l={}&start={}'.format(query, location, i)\n",
    "    try:\n",
    "        soup = get_soup(base_url)\n",
    "        urls += grab_job_links(soup)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_job_links(query):\n",
    "    \"\"\"Enter a search query and the function will output a list of URLs with the proper page information.\n",
    "    e.g. if there are 30 jobs there will only be URLs with page numbers going up to 30.\n",
    "    Were the page number in a search to go higher the last page would simply be returned repeatedly.\"\"\"\n",
    "    \n",
    "    urls = []\n",
    "    \n",
    "    num_postings_str = soup.find(name='div', attrs={'id':\"searchCount\"}).get_text().split()[-2]\n",
    "    num_postings_str = int(num_postings_str.replace(',',''))\n",
    "    num_postings_str\n",
    "    \n",
    "    for i in list(range(0,1000,10)):\n",
    "        base_url = 'https://www.indeed.com/jobs?q=data+scientist&l=New+York&start={}'.format(i)\n",
    "        soup = get_soup(base_url)\n",
    "    \n",
    "        for link in soup.find_all('h2', {'class': 'jobtitle'}):\n",
    "            try:\n",
    "                partial_url = link.a.get('href')\n",
    "                url = 'https://ca.indeed.com' + partial_url\n",
    "                urls.append(url)\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = grab_job_links('https://www.indeed.com/jobs?q=data+scientist&l=New+York&start={}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locations and key words on which to search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [['Data','Scientist'],['Machine','Learning','Engineer'],\n",
    "             ['Data','Engineer'],['Research','Engineer'],\n",
    "             ['Artificial','Intelligence','Engineer'],\n",
    "             ['Product','Analyst'],['Product','Scientist']]\n",
    "\n",
    "locations = [['Seattle'],['New','York,+NY&_ga='],\n",
    "             ['Los','Angeles'],['Chicago'],['San','Francisco','Bay','Area%2C+CA']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed will only return the first 1000 results when.  \n",
    "Some queries have more jobs but the same jobs will continue to be returned after 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_query = '+'.join(queries[0])\n",
    "s_location = '+'.join(locations[0])\n",
    "soup_url = 'https://www.indeed.com/jobs?q={}&l={}&start={}'.format(s_query, s_location, 0)\n",
    "print(soup_url)\n",
    "\n",
    "soup = get_soup(soup_url)\n",
    "\n",
    "num_postings_str = soup.find(name='div', attrs={'id':\"searchCount\"}).get_text().split()[-2]\n",
    "num_postings = int(num_postings_str.replace(',',''))\n",
    "\n",
    "\n",
    "\n",
    "if num_postings > 1000:\n",
    "    num_postings = 1000\n",
    "    \n",
    "range(0,num_postings,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'urls_to_query_DataScientist_Seattle'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_query = ''.join(queries[0])\n",
    "p_location = ''.join(locations[0])\n",
    "\n",
    "str('urls_to_query_' + '_'.join([p_query,p_location]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates list of urls that will be scraped to get actual job URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for location in locations:\n",
    "    \n",
    "    urls = []\n",
    "    \n",
    "    for query in queries:\n",
    "        \n",
    "        time.sleep(np.random.poisson(100)/50)\n",
    "    \n",
    "        s_query = '+'.join(query)\n",
    "        s_location = '+'.join(location)\n",
    "        soup_url = 'https://www.indeed.com/jobs?q={}&l={}&start={}'.format(s_query, s_location, 0)\n",
    "\n",
    "        soup = get_soup(soup_url)\n",
    "\n",
    "        num_postings_str = soup.find(name='div', attrs={'id':\"searchCount\"}).get_text().split()[-2]\n",
    "        num_postings = int(num_postings_str.replace(',',''))\n",
    "\n",
    "        if num_postings > 1000:\n",
    "            num_postings = 1000\n",
    "\n",
    "        for i in range(0,num_postings,10):\n",
    "            s_query = '+'.join(query)\n",
    "            s_location = '+'.join(location)\n",
    "            base_url = 'https://www.indeed.com/jobs?q={}&l={}&start={}'.format(s_query, s_location, i)\n",
    "            \n",
    "            urls.append(base_url)\n",
    "            \n",
    "    p_location = ''.join(location)\n",
    "    pickle_name = str('urls_to_query__' + p_location)\n",
    "\n",
    "    pickle_out = open(pickle_name,'wb')\n",
    "    pickle.dump(urls, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(np.random.poisson(100)/50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://ca.indeed.com/jobs?q={}&l={}&start={}'.format(query, location, num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"NewYork_DataScientist\",\"wb\")\n",
    "pickle.dump(urls, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open('NewYork_DataScientist','rb')\n",
    "NewYork_DataScientist = pickle.load(pickle_in)\n",
    "pickle_in.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(range(0,1000,10)):\n",
    "    base_url = 'https://www.indeed.com/jobs?q=data+scientist&l=New+York&start={}'.format(i)\n",
    "    try:\n",
    "        soup = get_soup(base_url)\n",
    "        urls += grab_job_links(soup)\n",
    "    except:\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
