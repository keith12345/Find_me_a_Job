{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    chrome_options = webdriver.chrome.options.Options()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    driver = webdriver.Chrome(executable_path='../../../anaconda3/bin/chromedriver',\n",
    "                              options=chrome_options)\n",
    "    driver.get(url)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    driver.close()\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the function you can run:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "soup = get_soup('https://www.indeed.com/jobs?q=data+scientist&l=New+York')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locations and key words on which to search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [['Data', 'Scientist'], ['Machine', 'Learning', 'Engineer'],\n",
    "             ['Data', 'Engineer'], ['Research', 'Engineer'],\n",
    "             ['Artificial', 'Intelligence', 'Engineer'],\n",
    "             ['Product', 'Analyst'], ['Product', 'Scientist']]\n",
    "\n",
    "locations = [['Seattle'], ['New', 'York,+NY&_ga='],\n",
    "             ['Los', 'Angeles'], ['Chicago'],\n",
    "             ['San', 'Francisco', 'Bay', 'Area%2C+CA']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed will only return the first 1000 results when.  \n",
    "Some queries have more jobs but the same jobs will continue to be returned after 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates list of urls that will be scraped to get actual job URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Failed on one attempt. Need to add a look to retry if soup.find(name='div', attrs={'id': \"searchCount\"}).get_text() returns as NoneType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_listings_pages(locations, queries):\n",
    "    \"\"\"\n",
    "    Given a list of locations and job titles, runs a query for each job title\n",
    "    for each location.\n",
    "    Then calls the get_soup() function for each location/job combination.\n",
    "    Grabs the number of results. On Indeed.com, if you specify page results higher\n",
    "    than 1000, it will continue to return page results for 1000, therefore, if the\n",
    "    reults are higher than 1000, it is capped.\n",
    "\n",
    "    Results are then pickled.\n",
    "    \"\"\"\n",
    "\n",
    "    for location in locations:\n",
    "\n",
    "        urls = []\n",
    "\n",
    "        for query in queries:\n",
    "\n",
    "            time.sleep(np.random.poisson(100)/50)\n",
    "\n",
    "            s_query = '+'.join(query)\n",
    "            s_location = '+'.join(location)\n",
    "            soup_url = 'https://www.indeed.com/jobs?q={}&l={}&start={}'.format(s_query, s_location, 0)\n",
    "\n",
    "            soup = get_soup(soup_url)\n",
    "\n",
    "            num_postings_str = soup.find(name='div', attrs={'id': \"searchCount\"}).get_text().split()[-2]\n",
    "            num_postings = int(num_postings_str.replace(',', ''))\n",
    "\n",
    "            if num_postings > 1000:\n",
    "                num_postings = 1000\n",
    "\n",
    "            for i in range(0,num_postings,10):\n",
    "                s_query = '+'.join(query)\n",
    "                s_location = '+'.join(location)\n",
    "                base_url = 'https://www.indeed.com/jobs?q={}&l={}&start={}'.format(s_query, s_location, i)\n",
    "\n",
    "                urls.append(base_url)\n",
    "\n",
    "        p_location = ''.join(location)\n",
    "        pickle_name = str('urls_to_query__' + p_location)\n",
    "\n",
    "        pickle_out = open(pickle_name,'wb')\n",
    "        pickle.dump(urls, pickle_out)\n",
    "        pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_job_listings_pages(locations, queries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
