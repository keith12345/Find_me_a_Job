{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import psycopg2\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.strftime(datetime.today(), \"%Y-%m-%d\")\n",
    "\n",
    "with open('conn.json') as fp:\n",
    "    conn_kwargs = json.load(fp)\n",
    "\n",
    "conn = psycopg2.connect(**conn_kwargs)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    chrome_options = webdriver.chrome.options.Options()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    driver = webdriver.Chrome(\n",
    "        executable_path='../../miniconda3/bin/chromedriver',\n",
    "        options=chrome_options)\n",
    "    driver.get(url)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    driver.close()\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_job_links(base_url):\n",
    "    \n",
    "    soup = get_soup(base_url)\n",
    "\n",
    "    urls = []\n",
    "\n",
    "    for link in soup.find_all('div', {'class': 'title'}):\n",
    "        try:\n",
    "            partial_url = link.a.get('href')\n",
    "            url = 'https://www.indeed.com' + partial_url\n",
    "            urls.append(url)\n",
    "            del url\n",
    "            del partial_url\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    del soup\n",
    "        \n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jobs_pages(cur):\n",
    "\n",
    "    date = str(datetime.strftime(datetime.today(), \"%Y-%m-%d\"))\n",
    "    \n",
    "    cur.execute(\"\"\"\n",
    "    SELECT DISTINCT(link), query, city\n",
    "    FROM job_listings_pages\n",
    "    WHERE city ILIKE 'San+Francisco+Bay+Area%2C+CA';\n",
    "    \"\"\")\n",
    "    job_listing_urls = cur.fetchall()\n",
    "    \n",
    "    for query, city, job_listing_url in job_listing_urls:\n",
    "        \n",
    "        urls = grab_job_links(job_listing_url)\n",
    "        \n",
    "        values = ((date, query, city, url) for url in urls)\n",
    "\n",
    "        insert_statement = '''\n",
    "        INSERT INTO jobs_pages\n",
    "        (date_pulled, query, city, link)\n",
    "        VALUES (%s, %s, %s, %s);\n",
    "        '''\n",
    "\n",
    "        cur.executemany(insert_statement, values)\n",
    "        \n",
    "        conn.commit()\n",
    "        \n",
    "        time.sleep(np.random.poisson(100)/50)\n",
    "    \n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_jobs_pages(cur)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
