{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import gensim\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('Data/clean_seattle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2908"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine documents and split into one list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sents = list(sent_to_words(df.listed_items))\n",
    "all_words = [item for sublist in clean_sents for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ngrams for corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "457518"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = ['_'.join(x) for x in zip(all_words, all_words[1:])]\n",
    "len(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['year_experi',\n",
       " 'comput_scienc',\n",
       " 'machin_learn',\n",
       " 'bachelor_degre',\n",
       " 'communic_skill']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_bigrams = pd.Series(bigrams).value_counts()[:100].index\n",
    "list(top_bigrams[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "457517"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams = ['_'.join(x) for x in zip(all_words, all_words[1:], all_words[2:])]\n",
    "len(trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like medic_dental_vision appears a lot... I'll make a note to get rid of that one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['degre_comput_scienc',\n",
       " 'comput_scienc_relat',\n",
       " 'bachelor_degre_comput',\n",
       " 'written_communic_skill',\n",
       " 'scienc_relat_field']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_trigrams = pd.Series(trigrams).value_counts()[:50].index\n",
    "list(top_trigrams[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quadgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "457516"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quadgrams = ['_'.join(x) for x in zip(all_words, all_words[1:], all_words[2:], all_words[3:])]\n",
    "len(quadgrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about finding a way to handle the fact that these and similar items are treated as three separate items:\n",
    "* verbal_written_communic_skill        203\n",
    "* written_verbal_communic_skill        194\n",
    "* excel_verbal_written_communic        121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bachelor_degre_comput_scienc',\n",
       " 'comput_scienc_relat_field',\n",
       " 'degre_comput_scienc_relat',\n",
       " 'verbal_written_communic_skill',\n",
       " 'written_verbal_communic_skill']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_quadgrams = pd.Series(quadgrams).value_counts().index[:20]\n",
    "list(top_quadgrams)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove redundant bigrams, trigrams, and quadgrams and combine to create a single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  56 non redundant bigrams\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['machin_learn',\n",
       " 'best_practic',\n",
       " 'project_manag',\n",
       " 'product_manag',\n",
       " 'experi_build']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_redundant_bigrams = []\n",
    "trigrams_string = ' '.join(top_trigrams)\n",
    "\n",
    "for bigram in top_bigrams:\n",
    "    if bigram in trigrams_string:\n",
    "        pass\n",
    "    else:\n",
    "        non_redundant_bigrams.append(bigram)\n",
    "print('There are ', len(non_redundant_bigrams), 'non-redundant bigrams')\n",
    "non_redundant_bigrams[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  30 non-redundant bigrams\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['problem_solv_skill',\n",
       " 'fast_pace_environ',\n",
       " 'year_relev_experi',\n",
       " 'communic_skill_abil',\n",
       " 'comput_scienc_fundament']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_redundant_trigrams = []\n",
    "quadgrams_string = ' '.join(top_quadgrams)\n",
    "\n",
    "for trigram in top_trigrams:\n",
    "    if trigram in quadgrams_string:\n",
    "        pass\n",
    "    else:\n",
    "        non_redundant_trigrams.append(trigram)\n",
    "print('There are ', len(non_redundant_trigrams), 'non-redundant bigrams')\n",
    "non_redundant_trigrams[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ngrams = ' '.join([' '.join(non_redundant_bigrams), ' '.join(non_redundant_trigrams), ' '.join(top_quadgrams)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert documents to ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ngram(text):\n",
    "    \n",
    "    out = []\n",
    "    \n",
    "    for i, j in zip(text.split(), text.split()[1:]):\n",
    "        bigram = '_'.join([i, j])\n",
    "        out.append(bigram)\n",
    "        \n",
    "    for i, j, k in zip(text.split(), text.split()[1:], text.split()[2:]):\n",
    "        trigram = '_'.join([i, j, k])\n",
    "        out.append(trigram)\n",
    "        \n",
    "    for i, j, k, l in zip(text.split(), text.split()[1:],\n",
    "                          text.split()[2:], text.split()[3:]):\n",
    "        quadgram = '_'.join([i, j, k, l])\n",
    "        out.append(quadgram) \n",
    "        \n",
    "    return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listed_items</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[develop_high, high_scalabl, scalabl_classifi,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[play_pivot, pivot_role, role_modern, modern_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[act_contribut, contribut_ux, ux_design, desig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[manag_6, 6_8, 8_technic, technic_product, pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[lead_grow, grow_appli, appli_scientist, scien...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        listed_items\n",
       "0  [develop_high, high_scalabl, scalabl_classifi,...\n",
       "1  [play_pivot, pivot_role, role_modern, modern_s...\n",
       "2  [act_contribut, contribut_ux, ux_design, desig...\n",
       "3  [manag_6, 6_8, 8_technic, technic_product, pro...\n",
       "4  [lead_grow, grow_appli, appli_scientist, scien..."
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_df = pd.DataFrame(df.listed_items.apply(to_ngram))\n",
    "ngram_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_matching_ngrams(doc_ngrams):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. create bigrams for corpus  \n",
    "* create bigrams for individual documents  \n",
    "* get value counts (using numpy???) of bigram appearances\n",
    "* check which bigrams appear most in the entire corpus - set arbitrary cutoff for number of bigrams based on quality\n",
    "* Remove redundant bigrams, trigrams, and quadgrams\n",
    "* check which of those selected bigrams are in each document\n",
    "    * to do this, find the intersection of a set for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "trigram = gensim.models.phrases.Phrases(bigram[all_words], min_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_model = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "develop high scalabl classifi tool leverag machin learn regress rule base model suggest collect synthes requir creat effect featur roadmap deliver tandem engin adapt standard machin learn method best exploit modern parallel environ distribut cluster multicor smp gpu ms degre comput scienc relat quantit field ph degre comput scienc relat quantit field year experi one follow area machin learn recommend system pattern recognit mine artifici intellig proven abil translat insight busi recommend experi hadoop hbase pig mapreduc sawzal bigtabl knowledg develop debug java experi script languag perl python php shell script experi filesystem server architectur distribut system \n",
      "\n",
      "play pivot role modern standard enhanc peoplesoft secur util modern industri best practic provid secur strategi guidelin develop lead strategi protect softwar secur issu infiltr attempt social engin threat drive secur busi process system reengin effort strong background softwar develop inform secur busi analysi experi larg integr enterpris system deep understand peoplesoft architectur technolog framework demonstr abil manag multipl complex initi fast pace product environ knowledg best practic secur best practic agil softwar develop practic system design secur implement techniqu qualiti assur process reengin custom focus build maintain posit relationship peer depart member flex adapt requir multifacet environ love technolog teamwork competit healthcar plan flexibl environ free transport option site gym manag support structur employe assist program paid vacat time addit paid holiday paid sick leav paid parent leav defer compens plan employ contribut state pension plan ten paid holiday two addit person holiday king counti websit seattl washington state posit repres local inform technolog posit classifi applic develop master \n",
      "\n",
      "act contribut ux design high visibl fortun client bring user center design ucd process method digit solut concept launch client understand busi requir broader context digit strategi advoc user develop intuit interact easi use interfac identifi execut best method user research test within client constraint design deliv inform architectur wirefram user persona journey prototyp optim wide rang devic interfac alongsid fellow design ensur design document creat holist client vision cross function need integr ideat co creat cross function teammat visual design develop engag manag busi technolog analyst pursuit produc best industri creativ intern client face project present creativ expert articul design process direct client oversight design leadership support creation facilit creativ workshop client clear passion enthusiasm learn pursu expertis constant challeng vocal definit make good user experi sale excel servic excel deliv sale servic strategi plan oper model definit incent design implement oper support digit enabl sale channel custom servic capabl digit experi creat engag omni channel digit experi across web mobil ar vr voic gestur iot video flexibl consumpt commerci strategi oper model definit capabl deliveri subscript flexibl consumpt busi model digit foundri flexibl end end deliveri model emphas innov disrupt digitalmix integr set platform enabl busi platform industri experi includ year relat design experi deep trench cross function team divers agenc product design onlin portfolio showcas experi storytel design present skill demonstr strong design judgement abil solv nebul ux problem showcas project breadth divers project challeng focus guid experi design process principl techniqu experi ux research includ plan design facilit user session client workshop analyz find articul result inform architectur user screen flow conceptu detail wirefram prototyp wide varieti modern design tool like sketch axur principl framer invis review exist system process identifi ux issu develop ux recommend abil approach design challeng open mind abil navig difficult relationship organ self direct get thing done deleg junior design execut communic idea effici persuas manner independ group need experi emerg technolog design trend tool method consist passion learn pursu craft strong written verbal communic skill willing travel percent experi agil methodolog qualit quantit research methodolog experi convers ui experi augment virtual mix realiti motion design anim particip wider design communiti connect creativ organ uxpa creativ morn etc \n",
      "\n",
      "manag technic product manag respons day day success lead definit execut roll featur custom face api collabor product manag softwar develop manag engin influenc roadmap adjac servic defin price servic develop detail crisp busi requir user stori present roadmap individu featur custom includ trade show execut brief bring servic featur market includ run beta program collabor product market industri analyst etc collabor learn world leader cloud comput storag servic year experi manag product manag year experi product manag year experi develop defin manag execut new product fast pace environ experi take ownership drive result experi financi model cost revenu experi defin product custom requir nas san object storag system ip network secur relat technolog bachelor degre comput scienc mathemat system engin mis relat field storag industri experi year system engin storag engin solut architect softwar engin year overal high tech industri experi experi product market sale busi develop deliv product market master busi administr mba equival experi meet exceed amazon leadership principl requir role meet exceed amazon function technic depth complex role \n",
      "\n",
      "lead grow appli scientist solv problem zillow offer domain area includ machin learn asset portfolio risk manag oper research develop lead long term vision portfolio research initi partner custom stakehold understand clarifi requir creativ solv complex busi problem analyt solut busi softwar engin team lead integr success model algorithm complex real time product system core zillow offer oper recruit hire mentor extraordinari appli machin learn scientist statistician phd comput scienc statist mathemat oper research relat disciplin year experi quantit technic field includ experi real world dataset year profession experi manag appli machin learn scientist abil distil inform custom requir problem definit deal ambigu compet object expert least one domain area oper research machin learn financi engin risk manag experi hire manag mentor appli scientist various level \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in clean_sents[0:5]:\n",
    "    print(f'{\" \".join(trigram_model[bigram_model[s]]) } \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>00am</th>\n",
       "      <th>00pm</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>100k</th>\n",
       "      <th>...</th>\n",
       "      <th>zero</th>\n",
       "      <th>zestim</th>\n",
       "      <th>zillow</th>\n",
       "      <th>zip</th>\n",
       "      <th>zipwhip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zulili</th>\n",
       "      <th>zunivers</th>\n",
       "      <th>ﬁling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6091 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  00am  00pm  08  09  10  100  1000  100k  ...    zero  zestim  \\\n",
       "0   0    0     0     0   0   0   0    0     0     0  ...       0       0   \n",
       "1   0    0     0     0   0   0   0    0     0     0  ...       0       0   \n",
       "2   0    0     0     0   0   0   0    1     0     0  ...       0       0   \n",
       "3   0    0     0     0   0   0   2    0     0     0  ...       0       0   \n",
       "4   0    0     0     0   0   0   0    0     0     0  ...       0       0   \n",
       "\n",
       "   zillow  zip  zipwhip  zone  zoom  zulili  zunivers  ﬁling  \n",
       "0       0    0        0     0     0       0         0      0  \n",
       "1       0    0        0     0     0       0         0      0  \n",
       "2       0    0        0     0     0       0         0      0  \n",
       "3       0    0        0     0     0       0         0      0  \n",
       "4       2    0        0     0     0       0         0      0  \n",
       "\n",
       "[5 rows x 6091 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(max_df=.95,min_df=2)\n",
    "cv_array = cv.fit_transform(df.listed_items).toarray()\n",
    "cv_df = pd.DataFrame(cv_array,columns=cv.get_feature_names())\n",
    "cv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(n_components=10, random_state=42)\n",
    "nmf = nmf_model.fit_transform(cv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = nmf\n",
    "H = nmf_model.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The W factor contains the document membership weights relative to each of the k topics. Each row corresponds to a single document, and each column correspond to a topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2908, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The H factor contains the term weights relative to each of the k topics. In this case, each row corresponds to a topic, and each column corresponds to a unique term in the corpus vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 6091)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_indices = np.argsort(H[1,:])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptor(terms, H, topic_index, top):\n",
    "    #reverse sort the values to sort the indices\n",
    "    top_indices = np.argsort(H[topic_index,:])[::-1]\n",
    "    top_terms = []\n",
    "    for term_index in top_indices[0:top]:\n",
    "        top_terms.append(terms[term_index])\n",
    "    return top_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "experi, year, tool, strong, etc, develop, environ, degre, knowledg, analysi\n",
      "Topic #1:\n",
      "busi, analyt, solut, develop, analysi, support, requir, partner, process, model\n",
      "Topic #2:\n",
      "system, support, oper, servic, manag, network, secur, technic, technolog, perform\n",
      "Topic #3:\n",
      "product, custom, market, manag, develop, engin, experi, sale, team, technic\n",
      "Topic #4:\n",
      "research, analysi, analyt, quantit, energi, design, complex, result, help, experi\n",
      "Topic #5:\n",
      "requir, may, document, posit, includ, educ, experi, applic, must, inform\n",
      "Topic #6:\n",
      "project, manag, develop, program, plan, process, report, resourc, includ, coordin\n",
      "Topic #7:\n",
      "design, test, develop, engin, softwar, system, product, requir, tool, process\n",
      "Topic #8:\n",
      "abil, skill, communic, strong, excel, demonstr, technic, written, effect, verbal\n",
      "Topic #9:\n",
      "learn, comput, scienc, machin, engin, model, softwar, build, develop, statist\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_top_words(nmf_model,cv.get_feature_names(),10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
