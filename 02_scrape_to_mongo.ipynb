{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import psycopg2\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_info(soup):\n",
    "\n",
    "    try:\n",
    "        job_title = soup.find(name='h3', attrs={\n",
    "            'class':'icl-u-xs-mb--xs icl-u-xs-mt--none \\\n",
    "            jobsearch-JobInfoHeader-title'}).text\n",
    "    except:\n",
    "        job_title = ''\n",
    "        \n",
    "    try:\n",
    "        company_name = soup.find(name='div', attrs={\n",
    "            'class':'icl-u-lg-mr--sm icl-u-xs-mr--xs'}).text\n",
    "    except:\n",
    "        company_name = ''\n",
    "        \n",
    "    try:\n",
    "        salary = soup.find(name='span', attrs={'class':'icl-u-xs-mr--xs'}).text\n",
    "    except:\n",
    "        salary = ''\n",
    "    \n",
    "    try:\n",
    "        job_desc_full = soup.find(name='div',attrs={\n",
    "            'class':'jobsearch-JobComponent-description icl-u-xs-mt--md'})\n",
    "        \n",
    "        try:\n",
    "            job_desc_full = soup.find(name='div', attrs={\n",
    "                'class':'jobsearch-JobComponent-description icl-u-xs-mt--md'})\n",
    "            listed_items = []\n",
    "            for list_item in job_desc_full.find_all(name='li'):\n",
    "                listed_items.append(list_item.text)\n",
    "            listed_items = '. '.join(listed_items)\n",
    "            listed_items = re.sub('\\n| +|\\.\\.', ' ', listed_items)\n",
    "        except:\n",
    "            listed_items = ''\n",
    "         \n",
    "        try:\n",
    "            paragraph_items = []\n",
    "            for paragraph_item in job_desc_full.find_all(name='p'):\n",
    "                paragraph_items.append(paragraph_item.text)\n",
    "\n",
    "            # Remove partially captured descriptions\n",
    "            paragraph_items = [paragraph_item for paragraph_item in\n",
    "                               paragraph_items if len(paragraph_item) > 50]\n",
    "            # Puts all paragraph items into a single string\n",
    "            paragraph_items = ''.join(paragraph_items)\n",
    "            paragraph_items = re.sub('\\n', ' ', paragraph_items)\n",
    "\n",
    "            if len(paragraph_items) < 1:\n",
    "                try:\n",
    "                    job_desc_full = soup.find(name='div',attrs={\n",
    "                        'class':\n",
    "                        'jobsearch-JobComponent-description icl-u-xs-mt--md'})\n",
    "                    paragraph_items = []\n",
    "                    for paragraph_item in job_desc_full.find_all(name='div'):\n",
    "                        paragraph_items.append(paragraph_item.text)\n",
    "\n",
    "                    paragraph_items = [paragraph_item for paragraph_item in\n",
    "                                       paragraph_items if len(paragraph_item)\n",
    "                                       > 50]\n",
    "                    paragraph_items = ''.join(paragraph_items)\n",
    "                    paragraph_items = re.sub('\\n', ' ', paragraph_items)\n",
    "                except: \n",
    "                    paragraph_items = ''\n",
    "   \n",
    "        except:\n",
    "            paragraph_items = ''\n",
    "    \n",
    "    except:\n",
    "        listed_items, paragraph_items = ['','']\n",
    "            \n",
    "    return job_title, company_name, salary, listed_items, paragraph_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chicago_Jobs',\n",
       " 'Los_Angeles_Jobs',\n",
       " 'SF_Bay_Area_Jobs',\n",
       " 'Seattle_Jobs',\n",
       " 'New_York_Jobs']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = MongoClient()\n",
    "db = client.jobs_data\n",
    "db.list_collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickles = [\n",
    "    'individual_listings__chicago',\n",
    "    'individual_listings__LosAngeles',\n",
    "    'individual_listings__NewYork',\n",
    "    'individual_listings__SanFranciscoBayArea',\n",
    "    'individual_listings__Seattle'\n",
    "]\n",
    "collections = [\n",
    "    'Chicago_Jobs',\n",
    "    'Los_Angeles_Jobs',\n",
    "    'New_York_Jobs',\n",
    "    'SF_Bay_Area_Jobs',\n",
    "    'Seattle_Jobs'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "date = datetime.strftime(datetime.today(), \"%Y-%m-%d\")\n",
    "\n",
    "with open('conn.json') as fp:\n",
    "    conn_kwargs = json.load(fp)\n",
    "\n",
    "conn = psycopg2.connect(**conn_kwargs)\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "SELECT DISTINCT(link), query, city\n",
    "FROM jobs_pages\n",
    "WHERE city ILIKE 'San+Francisco+Bay+Area%2C+CA';\"\"\")\n",
    "jobs_urls = cur.fetchall()\n",
    "\n",
    "for url, query, city in jobs_urls:\n",
    "\n",
    "    new_item = {}\n",
    "\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    job_title, company_name, salary, listed_items, paragraph_items \\\n",
    "    = get_job_info(soup)\n",
    "    time.sleep(np.random.poisson(100)/50)\n",
    "\n",
    "    new_item['date'] = date\n",
    "    new_item['url'] = url\n",
    "    new_item['query'] = query\n",
    "    new_item['city'] = city\n",
    "    new_item['job_title'] = job_title \n",
    "    new_item['company_name'] = company_name\n",
    "    new_item['salary'] = salary\n",
    "    new_item['listed_items'] = listed_items\n",
    "    new_item['paragraph_items'] = paragraph_items\n",
    "\n",
    "    time.sleep(np.random.poisson(100)/50)\n",
    "\n",
    "    db.SF_Bay_Area_Jobs.insert_one(new_item)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"\n",
    "SELECT DISTINCT(link), query, city\n",
    "FROM jobs_pages\n",
    "WHERE city ILIKE 'San+Francisco+Bay+Area%2C+CA';\n",
    "\"\"\")\n",
    "jobs_urls = cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11068"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jobs_urls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
